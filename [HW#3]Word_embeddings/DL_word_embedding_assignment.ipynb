{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "DL_word_embedding_assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hernanros/Y_Data_DL/blob/HW%233-answers/%5BHW%233%5DWord_embeddings/DL_word_embedding_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5MomtsDS6A",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding - Home Assigment\n",
        "## Dr. Omri Allouche 2018. YData Deep Learning Course\n",
        "\n",
        "[Open in Google Colab](https://colab.research.google.com/github/omriallouche/deep_learning_course/blob/master/DL_word_embedding_assignment.ipynb)\n",
        "    \n",
        "    \n",
        "In this exercise, you'll use word vectors trained on a corpus of 380,000 lyrics of songs from MetroLyrics (https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics).  \n",
        "The dataset contains these fields for each song, in CSV format:\n",
        "1. index\n",
        "1. song\n",
        "1. year\n",
        "1. artist\n",
        "1. genre\n",
        "1. lyrics\n",
        "\n",
        "Before doing this exercise, we recommend that you go over the \"Bag of words meets bag of popcorn\" tutorial (https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
        "\n",
        "Other recommended resources:\n",
        "- https://rare-technologies.com/word2vec-tutorial/\n",
        "- https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6V0oZAYDe0v",
        "colab_type": "code",
        "outputId": "016facfa-595e-4990-f6c1-a5913da9c834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "! git clone https://github.com/Hernanros/Y_data_DL DL"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DL'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 184 (delta 2), reused 2 (delta 0), pack-reused 176\u001b[K\n",
            "Receiving objects: 100% (184/184), 336.87 MiB | 13.97 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tezekQlprfdR",
        "colab_type": "code",
        "outputId": "634d92b7-87ed-4fce-a568-9dc1ff0daa6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "! git clone -b HW#3-answers https://github.com/Hernanros/Y_Data_DL DL-Branch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DL-Branch'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 184 (delta 2), reused 2 (delta 0), pack-reused 176\u001b[K\n",
            "Receiving objects: 100% (184/184), 336.87 MiB | 11.83 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "Checking out files: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoaovIPEDgpn",
        "colab_type": "code",
        "outputId": "3bc97aed-32ef-4c33-ad54-ee963976a9df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd /content/DL-Branch/[HW#3]Word_embeddings"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL-Branch/[HW#3]Word_embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DoWJedoHjBt",
        "colab_type": "code",
        "outputId": "65798dd5-8953-43bf-aefe-f2c9771fc7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.38.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I92xd9JiIpFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export KAGGLE_USERNAME=shaulsolomon\n",
        "!export KAGGLE_KEY='800b53729b9f95e4f150ac8e991674f0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbGtgJAAicWW",
        "colab_type": "code",
        "outputId": "c4f32039-0297-4b4b-b75d-1ecaf24404bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL-Branch/[HW#3]Word_embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFc1zUStMGMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "token = {\"username\":\"shaulsolomon\",\"key\":\"800b53729b9f95e4f150ac8e991674f0\"}\n",
        "with open('./kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDtt9YKjM6ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp ./kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZHZwvYNMtRW",
        "colab_type": "code",
        "outputId": "df1547c0-dade-478e-e1d5-84abf741cd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!kaggle config set -n path -v/content/DL-Branch/[HW#3]Word_embeddings"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- path is now set to: /content/DL-Branch/[HW#3]Word_embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFIFm2bENA9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJSTR2nRNCiO",
        "colab_type": "code",
        "outputId": "e796a9ee-cae5-4615-f3f9-b72e4ba0a80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle datasets list -s metrolyrics"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                     title                             size  lastUpdated          downloadCount  \n",
            "--------------------------------------  --------------------------------  ----  -------------------  -------------  \n",
            "gyani95/380000-lyrics-from-metrolyrics  380,000+ lyrics from MetroLyrics  96MB  2017-01-11 02:05:53           9782  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZw22naWO2t8",
        "colab_type": "code",
        "outputId": "79a6e03c-6ef4-4216-8c3b-e056d5e1ec50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download -d gyani95/380000-lyrics-from-metrolyrics"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 380000-lyrics-from-metrolyrics.zip to /content/DL-Branch/[HW#3]Word_embeddings/datasets/gyani95/380000-lyrics-from-metrolyrics\n",
            " 93% 89.0M/95.6M [00:00<00:00, 126MB/s]\n",
            "100% 95.6M/95.6M [00:00<00:00, 140MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyP7M9NvNf4y",
        "colab_type": "code",
        "outputId": "4f7be86e-87ff-4eb0-8092-01bbc1a02dfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip /content/DL-Branch/[HW#3]Word_embeddings/datasets/gyani95/380000-lyrics-from-metrolyrics/380000-lyrics-from-metrolyrics.zip"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/DL-Branch/[HW#3]Word_embeddings/datasets/gyani95/380000-lyrics-from-metrolyrics/380000-lyrics-from-metrolyrics.zip\n",
            "  inflating: lyrics.csv              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVPzkBE2Nmgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB9n5HQ5DS6E",
        "colab_type": "text"
      },
      "source": [
        "### Train word vectors\n",
        "Train word vectors using the Skipgram Word2vec algorithm and the gensim package.\n",
        "Make sure you perform the following:\n",
        "- Tokenize words\n",
        "- Lowercase all words\n",
        "- Remove punctuation marks\n",
        "- Remove rare words\n",
        "- Remove stopwords\n",
        "\n",
        "Use 300 as the dimension of the word vectors. Try different context sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G-kPmlzNlVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"lyrics.csv\")\n",
        "data = data.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZosLwhS5OdFc",
        "colab_type": "code",
        "outputId": "85f88483-5427-4737-fdb2-b4aac8b4b044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#idx=np.random.randint(0,len(data),size=5)\n",
        "data.head()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>song</th>\n",
              "      <th>year</th>\n",
              "      <th>artist</th>\n",
              "      <th>genre</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>clean_lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>355340</th>\n",
              "      <td>355340</td>\n",
              "      <td>be-my-guest</td>\n",
              "      <td>2012</td>\n",
              "      <td>eurovision</td>\n",
              "      <td>Metal</td>\n",
              "      <td>Eurovision - Be My Guest\\nEurovision - Be My G...</td>\n",
              "      <td>[eurovision, be, my, guest, eurovision, be, my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295283</th>\n",
              "      <td>295283</td>\n",
              "      <td>como-se-sente</td>\n",
              "      <td>2013</td>\n",
              "      <td>capital-inicial</td>\n",
              "      <td>Rock</td>\n",
              "      <td>Como se sente\\nDe volta ao comeÃ§o\\nAs falhas,...</td>\n",
              "      <td>[como, se, sente, de, volta, ao, come, o, as, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162863</th>\n",
              "      <td>162863</td>\n",
              "      <td>one-by-one</td>\n",
              "      <td>2007</td>\n",
              "      <td>billy-bragg</td>\n",
              "      <td>Rock</td>\n",
              "      <td>One by one the teardrops fall as I write to yo...</td>\n",
              "      <td>[one, teardrops, fall, i, write, one, words, c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189609</th>\n",
              "      <td>189609</td>\n",
              "      <td>wishful-thinking</td>\n",
              "      <td>2001</td>\n",
              "      <td>case</td>\n",
              "      <td>Hip-Hop</td>\n",
              "      <td>Ooh, oh babe, ooh yeah\\nThere's a reason\\nWhy ...</td>\n",
              "      <td>[ooh, oh, babe, ooh, yeah, there, s, reason, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102013</th>\n",
              "      <td>102013</td>\n",
              "      <td>dumping-the-body</td>\n",
              "      <td>2012</td>\n",
              "      <td>danny-elfman</td>\n",
              "      <td>Not Available</td>\n",
              "      <td>INSTRUMENTAL</td>\n",
              "      <td>[instrumental]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         index  ...                                       clean_lyrics\n",
              "355340  355340  ...  [eurovision, be, my, guest, eurovision, be, my...\n",
              "295283  295283  ...  [como, se, sente, de, volta, ao, come, o, as, ...\n",
              "162863  162863  ...  [one, teardrops, fall, i, write, one, words, c...\n",
              "189609  189609  ...  [ooh, oh, babe, ooh, yeah, there, s, reason, w...\n",
              "102013  102013  ...                                     [instrumental]\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpeXJj93WuaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "9c50de9b-b8d5-44ba-a2b3-4c7f3355a071"
      },
      "source": [
        "lyrics_to_words(data.lyrics[0])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['oh', 'baby'],\n",
              " ['you', 'know', 'i', 'm', 'gonna', 'cut', 'right', 'chase'],\n",
              " ['some', 'women'],\n",
              " ['i', 'like', 'think', 'i', 'created', 'special', 'purpose'],\n",
              " ['you', 'know', 's', 'special', 'you', 'feel'],\n",
              " ['it', 's', 'baby', 'let', 's', 'lost'],\n",
              " ['you', 't', 'need', 'work', 'cause', 'boss'],\n",
              " ['for', 'real', 'want', 'feel'],\n",
              " ['i', 'consider', 'lucky', 's', 'big', 'deal'],\n",
              " ['why', 'well', 'got', 'key', 'heart'],\n",
              " ['but', 'ain', 't', 'gonna', 'need', 'i', 'd', 'open', 'body'],\n",
              " ['and', 'secrets', 't', 'know', 'inside'],\n",
              " ['no', 'need', 'lie'],\n",
              " ['it', 's', 'big', 's', 'wide'],\n",
              " ['it', 's', 'strong', 'won', 't', 'fit'],\n",
              " ['it', 's', 's', 'tough'],\n",
              " ['he', 'talk', 'like', 'cause'],\n",
              " ['he', 'got', 'big', 'ego', 'huge', 'ego'],\n",
              " ['i', 'love', 'big', 'ego', 's'],\n",
              " ['he', 'walk', 'like', 'cause'],\n",
              " ['usually', 'i', 'm', 'humble', 'right', 'i', 't', 'choose'],\n",
              " ['you', 'leave', 'blues'],\n",
              " ['some', 'arrogant', 'i', 'confident'],\n",
              " ['you', 'decide', 'i', 'm', 'working'],\n",
              " ['damn', 'i', 'know', 'i', 'm', 'killing', 'legs'],\n",
              " ['better', 'thighs'],\n",
              " ['matter', 'fact', 's', 'smile', 'maybe', 'eyes'],\n",
              " ['boy', 'site', 'kind', 'like'],\n",
              " ['it', 's', 'big', 's', 'wide'],\n",
              " ['it', 's', 'strong', 'won', 't', 'fit'],\n",
              " ['it', 's', 's', 'tough'],\n",
              " ['i', 'talk', 'like', 'cause', 'i'],\n",
              " ['i', 'got', 'big', 'ego', 'huge', 'ego'],\n",
              " ['but', 'love', 'big', 'ego', 's'],\n",
              " ['i', 'walk', 'like', 'cause', 'i'],\n",
              " ['i', 'i', 'walk', 'like', 'cause', 'i'],\n",
              " ['i', 'i', 'talk', 'like', 'cause', 'i'],\n",
              " ['i', 'i', 'i'],\n",
              " ['i', 'walk', 'like', 'cause', 'i'],\n",
              " ['it', 's', 'big', 's', 'wide'],\n",
              " ['it', 's', 'strong', 'won', 't', 'fit'],\n",
              " ['it', 's', 's', 'tough'],\n",
              " ['he', 'talk', 'like', 'cause'],\n",
              " ['he', 'got', 'big', 'ego', 'huge', 'ego', 'huge', 'ego'],\n",
              " ['i', 'love', 'big', 'ego', 's'],\n",
              " ['he', 'walk', 'like', 'cause'],\n",
              " ['ego', 'big', 'admit'],\n",
              " ['i', 'got', 'reason', 'feel', 'like', 'i', 'm', 'bitch'],\n",
              " ['ego', 'strong', 'ain', 't', 'know'],\n",
              " ['i', 't', 'need', 'beat', 'i', 'sing', 'piano']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msNBsfJwUte3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def split_sentences (song):\n",
        "  '''\n",
        "  Function to create a sequence of words for each lyrics entry of our dataset.\n",
        "  insert - long string\n",
        "  out - list of lists \n",
        "  '''\n",
        "  listofsents=[]\n",
        "  for sent in song.split('\\n'):\n",
        "    if len (sent)>0:\n",
        "      listofsents.append (sent)\n",
        "  return listofsents\n",
        "\n",
        "def lyrics_to_words(lyrics):\n",
        "    # 1. split each text entry into sentences\n",
        "    sentences= split_sentences(lyrics)\n",
        "\n",
        "    # 2. parse through each sentence seperatley and store words as a sequence\n",
        "    wordlists=[]\n",
        "    for sent in sentences:\n",
        "    \n",
        "      # 3. Remove non-letters (punctuation)   \n",
        "      letters_only = re.sub(\"[^a-zA-Z]\", \" \", sent)\n",
        "\n",
        "      # 4. Remove stopwords\n",
        "      no_stop_words = remove_stopwords(letters_only) \n",
        "      \n",
        "      # 5. Convert to lower case, split into individual words\n",
        "      words = no_stop_words.lower().split()                               \n",
        "      #\n",
        "      wordlists+=words\n",
        "\n",
        "    return wordlists "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7qhGk2uWZid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['clean_lyrics'] = data.lyrics.apply(lambda x: lyrics_to_words(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWOyAIpZTy93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8dc8e55-2c52-4def-d44d-5b7857900027"
      },
      "source": [
        "data['clean_lyrics'][18]"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ay',\n",
              " 'ay',\n",
              " 'ay',\n",
              " 'nobody',\n",
              " 'likes',\n",
              " 'played',\n",
              " 'oh',\n",
              " 'beyonce',\n",
              " 'beyonce',\n",
              " 'oh',\n",
              " 'shakira',\n",
              " 'shakira',\n",
              " 'hey',\n",
              " 'he',\n",
              " 'said',\n",
              " 'i',\n",
              " 'm',\n",
              " 'worth',\n",
              " 'desire',\n",
              " 'i',\n",
              " 'know',\n",
              " 'things',\n",
              " 'wouldn',\n",
              " 't',\n",
              " 'wanna',\n",
              " 'read',\n",
              " 'he',\n",
              " 'kissed',\n",
              " 'this',\n",
              " 'beautiful',\n",
              " 'liar',\n",
              " 'tell',\n",
              " 'tolerate',\n",
              " 'things',\n",
              " 'you',\n",
              " 'll',\n",
              " 'know',\n",
              " 'why',\n",
              " 'ones',\n",
              " 'suffer',\n",
              " 'have',\n",
              " 'let',\n",
              " 'he',\n",
              " 'won',\n",
              " 't',\n",
              " 'ay',\n",
              " 'let',\n",
              " 's',\n",
              " 'kill',\n",
              " 'karma',\n",
              " 'ay',\n",
              " 'let',\n",
              " 's',\n",
              " 'start',\n",
              " 'fight',\n",
              " 'ay',\n",
              " 'it',\n",
              " 's',\n",
              " 'worth',\n",
              " 'drama',\n",
              " 'for',\n",
              " 'beautiful',\n",
              " 'liar',\n",
              " 'oh',\n",
              " 'can',\n",
              " 't',\n",
              " 'laugh',\n",
              " 'ha',\n",
              " 'ha',\n",
              " 'ha',\n",
              " 'oh',\n",
              " 'it',\n",
              " 's',\n",
              " 'worth',\n",
              " 'time',\n",
              " 'oh',\n",
              " 'we',\n",
              " 'live',\n",
              " 'just',\n",
              " 'beautiful',\n",
              " 'liar',\n",
              " 'i',\n",
              " 'trusted',\n",
              " 'but',\n",
              " 'i',\n",
              " 'followed',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'i',\n",
              " 't',\n",
              " 'know',\n",
              " 'till',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'i',\n",
              " 'walked',\n",
              " 'love',\n",
              " 'scene',\n",
              " 'slow',\n",
              " 'dancing',\n",
              " 'you',\n",
              " 'stole',\n",
              " 'i',\n",
              " 'wrong',\n",
              " 'we',\n",
              " 'know',\n",
              " 'when',\n",
              " 'pain',\n",
              " 'heartbreaks',\n",
              " 'have',\n",
              " 'let',\n",
              " 'the',\n",
              " 'innocence',\n",
              " 'gone',\n",
              " 'ay',\n",
              " 'let',\n",
              " 's',\n",
              " 'kill',\n",
              " 'karma',\n",
              " 'ay',\n",
              " 'let',\n",
              " 's',\n",
              " 'start',\n",
              " 'fight',\n",
              " 'ay',\n",
              " 'it',\n",
              " 's',\n",
              " 'worth',\n",
              " 'drama',\n",
              " 'for',\n",
              " 'beautiful',\n",
              " 'liar',\n",
              " 'oh',\n",
              " 'can',\n",
              " 't',\n",
              " 'laugh',\n",
              " 'ha',\n",
              " 'ha',\n",
              " 'ha',\n",
              " 'oh',\n",
              " 'it',\n",
              " 's',\n",
              " 'worth',\n",
              " 'time',\n",
              " 'oh',\n",
              " 'we',\n",
              " 'live',\n",
              " 'just',\n",
              " 'beautiful',\n",
              " 'liar',\n",
              " 'tell',\n",
              " 'forgive',\n",
              " 'when',\n",
              " 's',\n",
              " 's',\n",
              " 'ashamed',\n",
              " 'and',\n",
              " 'i',\n",
              " 'wish',\n",
              " 'i',\n",
              " 'free',\n",
              " 'of',\n",
              " 'hurt',\n",
              " 'pain',\n",
              " 'but',\n",
              " 'answer',\n",
              " 'simple',\n",
              " 'he',\n",
              " 's',\n",
              " 'blame',\n",
              " 'hey',\n",
              " 'ay',\n",
              " 'beyonce',\n",
              " 'beyonce',\n",
              " 'ay',\n",
              " 'shakira',\n",
              " 'shakira',\n",
              " 'oh',\n",
              " 'beyonce',\n",
              " 'beyonce',\n",
              " 'oh',\n",
              " 'shakira',\n",
              " 'shakira',\n",
              " 'hey',\n",
              " 'ay',\n",
              " 'let',\n",
              " 's',\n",
              " 'kill',\n",
              " 'karma',\n",
              " 'ay',\n",
              " 'let',\n",
              " 's',\n",
              " 'start',\n",
              " 'fight',\n",
              " 'ay',\n",
              " 'it',\n",
              " 's',\n",
              " 'worth',\n",
              " 'drama',\n",
              " 'for',\n",
              " 'beautiful',\n",
              " 'liar',\n",
              " 'oh',\n",
              " 'can',\n",
              " 't',\n",
              " 'laugh',\n",
              " 'ha',\n",
              " 'ha',\n",
              " 'ha',\n",
              " 'oh',\n",
              " 'it',\n",
              " 's',\n",
              " 'worth',\n",
              " 'time',\n",
              " 'oh',\n",
              " 'we',\n",
              " 'live',\n",
              " 'just',\n",
              " 'beautiful',\n",
              " 'liar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOuRJyD-XHD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setting sg=1 means skip-gram model ,sg=0 means CBOW model \n",
        "model = Word2Vec(data[\"clean_lyrics\"], size=300, min_count=20, workers=4, sg=1,window = 10 ,sample = 1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGF634ArDS6l",
        "colab_type": "text"
      },
      "source": [
        "### Review most similar words\n",
        "Get initial evaluation of the word vectors by analyzing the most similar words for a few interesting words in the text. \n",
        "\n",
        "Choose words yourself, and find the most similar words to them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n_qtfHtPv0F",
        "colab_type": "code",
        "outputId": "7f5ee51b-3ff1-4bda-d058-d1d86a6760e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model.save('basic1W2V_model')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HULsSJ0kq7Fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git remote set-url origin https://Hernanros:her321nan8011@github.com/Hernanros/Y_Data_DL.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8tOJ2K0wBm2",
        "colab_type": "code",
        "outputId": "2543dce8-6dda-47ae-e3e9-4d2dff9fc5e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git branch"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* \u001b[32mHW#3-answers\u001b[m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7RNlteGphkz",
        "colab_type": "code",
        "outputId": "f5761095-0e67-4ec5-f443-c0429fbf4e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "#!git config --global user.email \"hernan.rosenblum89@gmail.com\"\n",
        "#!git config --global user.name \"Hernanros\"\n",
        "#!git config --global user.password \"her321nan8011\"\n",
        "#! git add './basic1W2V_model.trainables.syn1neg.npy'\n",
        "#! git add './basic1W2V_model.wv.vectors.npy'\n",
        "#! git add './basic1W2V_model'\n",
        "#! git commit -m \"improved Word2Vec Model.Model\"\n",
        "! git push origin HW#3-answers"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting objects: 6, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (6/6), done.\n",
            "Writing objects: 100% (6/6), 97.77 MiB | 5.79 MiB/s, done.\n",
            "Total 6 (delta 2), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "remote: warning: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n",
            "remote: warning: See http://git.io/iEPt8g for more information.\u001b[K\n",
            "remote: warning: File [HW#3]Word_embeddings/basic1W2V_model.trainables.syn1neg.npy is 52.24 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "remote: warning: File [HW#3]Word_embeddings/basic1W2V_model.wv.vectors.npy is 52.24 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n",
            "To https://github.com/Hernanros/Y_Data_DL.git\n",
            "   589c95b..99a0296  HW#3-answers -> HW#3-answers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzwxkzEDS6o",
        "colab_type": "code",
        "outputId": "bd6915c5-95c6-4274-9297-89098560f14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model.most_similar('bitch')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nigga', 0.7136905193328857),\n",
              " ('shit', 0.6971545815467834),\n",
              " ('fuck', 0.6677021384239197),\n",
              " ('ass', 0.6604837775230408),\n",
              " ('bitches', 0.6412915587425232),\n",
              " ('niggas', 0.6216423511505127),\n",
              " ('fuckin', 0.6211873888969421),\n",
              " ('hoe', 0.6017568111419678),\n",
              " ('dick', 0.5984923839569092),\n",
              " ('hoes', 0.592513918876648)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLuvqYhshjeR",
        "colab_type": "code",
        "outputId": "66875699-1b1d-4b61-81da-d7b7e0dc383c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model.most_similar('cell')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jail', 0.59886634349823),\n",
              " ('prison', 0.5866299271583557),\n",
              " ('phone', 0.5229048728942871),\n",
              " ('locked', 0.518201470375061),\n",
              " ('bail', 0.5037834644317627),\n",
              " ('phones', 0.498323917388916),\n",
              " ('commissary', 0.4943329989910126),\n",
              " ('inmate', 0.49243199825286865),\n",
              " ('padded', 0.48261523246765137),\n",
              " ('cells', 0.4581572115421295)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaIXFCdvh8g3",
        "colab_type": "code",
        "outputId": "0c4da19e-73a2-4447-a213-c50f3ce4b03c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model.most_similar('muslim')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('buddhist', 0.5552605390548706),\n",
              " ('islam', 0.54984450340271),\n",
              " ('hindu', 0.5280613899230957),\n",
              " ('israelite', 0.4875798523426056),\n",
              " ('muslims', 0.48744964599609375),\n",
              " ('reverends', 0.48113059997558594),\n",
              " ('allah', 0.4554073214530945),\n",
              " ('democrat', 0.4503772258758545),\n",
              " ('atheists', 0.4492923617362976),\n",
              " ('republicans', 0.4486233592033386)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBRnI3cEiPsO",
        "colab_type": "code",
        "outputId": "0fefff4b-6740-4b6e-c29a-f701d228bf1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model.most_similar(positive=['christian','jewish','muslim'],negative='buddhist')"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('christianity', 0.12208638340234756),\n",
              " ('hebrew', 0.09629076719284058),\n",
              " ('rabble', 0.09438511729240417),\n",
              " ('apparel', 0.09437333047389984),\n",
              " ('traditions', 0.09244365990161896),\n",
              " ('praising', 0.09095346182584763),\n",
              " ('messengers', 0.08943312615156174),\n",
              " ('creed', 0.08820222318172455),\n",
              " ('psalms', 0.08716802299022675),\n",
              " ('senate', 0.08256478607654572)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG9LaY_Sg9sb",
        "colab_type": "code",
        "outputId": "c362372c-de73-4578-8c88-0466641575ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model.most_similar('jew')"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('jewish', 0.5424187183380127),\n",
              " ('celebrates', 0.47851115465164185),\n",
              " ('buddhist', 0.4746258556842804),\n",
              " ('yarmulke', 0.44507861137390137),\n",
              " ('jews', 0.4411275386810303),\n",
              " ('communist', 0.4321817457675934),\n",
              " ('mohammed', 0.42999032139778137),\n",
              " ('simpson', 0.42682984471321106),\n",
              " ('mitzvah', 0.42335399985313416),\n",
              " ('abdul', 0.4210525155067444)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-fQIWoFDS6z",
        "colab_type": "text"
      },
      "source": [
        "### Word Vectors Algebra\n",
        "We've seen in class examples of algebraic games on the word vectors (e.g. man - woman + king = queen ). \n",
        "\n",
        "Try a few vector algebra terms, and evaluate how well they work. Try to use the Cosine distance and compare it to the Euclidean distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paExw1cTkefL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''HELPER FUNCTIONS'''\n",
        "\n",
        "def closest_term(model,poswords,negwords=[], dist_func = cosine_similarity,vocab=model.wv.vocab):\n",
        "  tot_words=poswords+negwords\n",
        "  vector=np.zeros(model.wv.vector_size)\n",
        "  for word in poswords:\n",
        "    vector=np.add(vector,model.wv.get_vector(word))\n",
        "  if len(negwords)>0:\n",
        "    for word in negwords:\n",
        "      vector = np.subtract(vector, model.wv.get_vector(word))\n",
        "\n",
        "  closest_dist = 1000\n",
        "  most_sim=0\n",
        "  closest_word = None\n",
        "  for item in vocab:\n",
        "    dist = dist_func(model.wv.get_vector(item).reshape(1,-1),vector.reshape(1,-1))\n",
        "    if dist_func==cosine_similarity:\n",
        "      if (np.abs(dist) > np.abs(most_sim)) and(item not in tot_words):  \n",
        "        closest_word = item\n",
        "        most_sim = dist\n",
        "    else:\n",
        "      if (np.abs(dist) < np.abs(closest_dist)) and(item not in tot_words):\n",
        "        closest_word = item\n",
        "        closest_dist = dist\n",
        "  return closest_word\n",
        "\n",
        "def closest_n_words(model,vector,n=5, dist_func = cosine_similarity,vocab=model.wv.vocab):\n",
        "  dist=np.zeros(len(vocab))\n",
        "  keys=[]\n",
        "  for i,item in enumerate(vocab.keys()):\n",
        "    dist[i] = dist_func(model.wv.get_vector(item).reshape(1,-1),vector.reshape(1,-1))\n",
        "    keys.append(item)\n",
        "  sorted=np.argsort(dist)\n",
        "  return keys[:n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrVUmfUNsTHz",
        "colab_type": "code",
        "outputId": "c2e52d1a-fbf0-49cf-c75d-7af81077f193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos = ['money','fame','power']\n",
        "print(\"Euclidean distance = {}\\tcosine similarity = {}\".format( closest_term(model, ['money','fame','power'], dist_func = euclidean_distances),closest_term(model, ['money','fame','power'])))"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Euclidean distance = wealth\tcosine similarity = wealth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86aWGUJcyiMH",
        "colab_type": "code",
        "outputId": "2f477acf-a1ce-4c07-de06-9a6989ec1bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos,neg=['man','woman','sex'],['love']\n",
        "print(\"Euclidean distance = {}\\tcosine similarity = {}\".format( closest_term(model, pos,neg, dist_func = euclidean_distances),closest_term(model,pos,neg)))\n"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Euclidean distance = women\tcosine similarity = women\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_03L-76HpnVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6198c87-0965-4986-a0dd-97b3404b48fc"
      },
      "source": [
        "pos=['summer','tan','sun','play']\n",
        "print(\"Euclidean distance = {}\\tcosine similarity = {}\".format( closest_term(model, pos, dist_func = euclidean_distances),closest_term(model,pos)))\n"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Euclidean distance = rays\tcosine similarity = balmy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0DQsYJtAx5Q",
        "colab_type": "text"
      },
      "source": [
        "# Not sure we need this part anymore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5X7aBx8zRB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Word2Vec(data[\"clean_lyrics\"], size=50, min_count=10, workers=4, sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EbxVeG23v5w",
        "colab_type": "code",
        "outputId": "2d65bba9-0815-4eb7-aa06-341714f07541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "test_term = model2['thug'] + model2['money'] + model2['fame'] + model2['bitches']\n",
        "print(\"Euclidean distance = {}\\tcosine similarity = {}\".format( closest_term(model2, test_term, dist_func = euclidean_distances),closest_term(model2, test_term)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Euclidean distance = playerizm\tcosine similarity = sirena\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk78ATAs5Ivd",
        "colab_type": "code",
        "outputId": "6b9a9d09-8ac9-43c2-d4df-e4085711760b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "test_term = model2['thug'] + model2['money'] + model2['fame'] + model2['bitches'] - model2['shame'] + model2['crime'] - model2['weak']\n",
        "print(\"Euclidean distance = {}\\tcosine similarity = {}\".format( closest_term(model2, test_term, dist_func = euclidean_distances),closest_term(model2, test_term)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Euclidean distance = dolla\tcosine similarity = salome\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwYlOvm36aAY",
        "colab_type": "code",
        "outputId": "a5307f60-63b7-4304-d8ed-9c8b932911d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "test_term = model2['prison'] + model2['god'] + model2['pray'] - model2['sin'] + model2['salvation'] - model2['slavery'] - model2['walls']\n",
        "print(\"Euclidean distance = {}\\tcosine similarity = {}\".format( closest_term(model2, test_term, dist_func = euclidean_distances),closest_term(model2, test_term)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Euclidean distance = thank\tcosine similarity = pienso\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A4MgH337Qya",
        "colab_type": "text"
      },
      "source": [
        "We tried finding vectors based off the 300 dim model, and found that the closest words are the words used in the vector - which makes sense taking into account the 'Curse of Dimensionality'. So we tried training the model again with just 50 dimensions, and while it got better results, it still lacked a certain level of consistency.\n",
        "\n",
        "As such, it seems quite reasonable that with a very small dimensionality it will produce better results but then it will also give more scattered solutions as low dimensionality isnt good enough to properly capture the variance between dif. tokens in the dictionary.\n",
        "\n",
        "Therefore - using vector algebra seems like a great thought experiment to help explain ideas in slides, but quite impractical for our dataset. Perhaps with a more corpus that is more reflecting of natural language we could extract better results from vector algebra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBM32VJDDS67",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis\n",
        "Estimate sentiment of words using word vectors.  \n",
        "In this section, we'll use the SemEval-2015 English Twitter Sentiment Lexicon.  \n",
        "The lexicon was used as an official test set in the SemEval-2015 shared Task #10: Subtask E, and contains a polarity score for words in range -1 (negative) to 1 (positive) - http://saifmohammad.com/WebPages/SCL.html#OPP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1Ty2Lk89M4l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9b460796-03d8-4075-f9c5-27cfccc2ea76"
      },
      "source": [
        "! wget http://saifmohammad.com/WebDocs/lexiconstoreleaseonsclpage/SCL-OPP.zip"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-05 15:28:16--  http://saifmohammad.com/WebDocs/lexiconstoreleaseonsclpage/SCL-OPP.zip\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15221 (15K) [application/zip]\n",
            "Saving to: ‘SCL-OPP.zip’\n",
            "\n",
            "SCL-OPP.zip         100%[===================>]  14.86K  86.5KB/s    in 0.2s    \n",
            "\n",
            "2020-04-05 15:28:17 (86.5 KB/s) - ‘SCL-OPP.zip’ saved [15221/15221]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgxusUcaAuub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "20ec2ff4-d0de-4944-d3bf-1325b5fe004d"
      },
      "source": [
        "!unzip /content/DL-Branch/[HW#3]Word_embeddings/SCL-OPP.zip"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/DL-Branch/[HW#3]Word_embeddings/SCL-OPP.zip\n",
            "   creating: SCL-OPP/\n",
            "  inflating: SCL-OPP/readme.txt      \n",
            "  inflating: SCL-OPP/SCL-OPP.txt     \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/SCL-OPP/\n",
            "  inflating: __MACOSX/SCL-OPP/._SCL-OPP.txt  \n",
            "  inflating: __MACOSX/._SCL-OPP      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXqFFYSXDS68",
        "colab_type": "text"
      },
      "source": [
        "Build a classifier for the sentiment of a word given its word vector. Split the data to a train and test sets, and report the model performance on both sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndVVexzjDS6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_words = {}\n",
        "\n",
        "with open(\"./SCL-OPP/SCL-OPP.txt\",\"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    word = line.split(\"\\t\")\n",
        "    dict_words[word[0]] = float(word[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBhPX_DTR41L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#All the words in our lyrics corpus\n",
        "model_vocab =  list(model.wv.vocab.keys())\n",
        "#All the words shared between the sentiment lexicon and our lyrics corpus.\n",
        "lexicon_words = []\n",
        "for key,value in dict_words.items():\n",
        "  if key in model_vocab:\n",
        "    lexicon_words.append([key,value])\n",
        "lexicon_df = pd.DataFrame(lexicon_words, columns=[\"Word\",\"Sentiment\"])\n",
        "lexicon_dict=dict(lexicon_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY1pyW-zLuUT",
        "colab_type": "code",
        "outputId": "4e783e3c-2eb5-4fb5-e425-5f61aa08e85a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(model[lexicon_df.Word],lexicon_df.Sentiment, test_size = 0.2, random_state=42)\n",
        "\n",
        "ridge = RidgeCV(alphas=[0.01,.1,1,5,10],cv=5)\n",
        "ridge.fit(X_train,y_train)\n",
        "  \n",
        "\n",
        "lasso=LassoCV(alphas=[.0001,.0005,.001,.005,.01,.05],cv=5)\n",
        "lasso.fit(X_train,y_train)\n",
        "\n",
        "elastic=ElasticNetCV(alphas=[0.01,.1,1,5,10],cv=5)\n",
        "elastic.fit(X_train,y_train)\n",
        "\n",
        "print(\"Train Score:\\nRidge {},\\tLasso:{},\\tElastic net:{}\\n Test Score:\\nRidge {},\\tLasso:{},\\tElastic net:{}\".format(mse(ridge.predict(X_train),y_train),mse(lasso.predict(X_train),y_train),mse(elastic.predict(X_train),y_train),\n",
        "                                                                                  mse(ridge.predict(X_test),y_test),mse(lasso.predict(X_test),y_test),mse(elastic.predict(X_test),y_test))) \n"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 81\n",
            "Train Score:\n",
            "Ridge 0.05968776191881755,\tLasso:0.1014317827269261,\tElastic net:0.10481114821716832\n",
            " Test Score:\n",
            "Ridge 0.14526513720908418,\tLasso:0.17575739929469814,\tElastic net:0.17433719195261796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN0lWNilCMxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9WPuZJKDS7E",
        "colab_type": "text"
      },
      "source": [
        "**Use** your trained model from the previous question to predict the sentiment score of words in the lyrics corpus that are not part of the original sentiment dataset. Review the words with the highest positive and negative sentiment. Do the results make sense?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SO7ejIPYyfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3abc2b69-8f09-41ca-a3d9-d6487b7dbc04"
      },
      "source": [
        "#the next part will be to use our lr model to predict sentiment score for any word embeddings\n",
        "not_in_lexicon = []\n",
        "for word in model_vocab:\n",
        "  if word not in lexicon_df['Word'].values:\n",
        "    not_in_lexicon.append(word)\n",
        "\n",
        "values = elastic.predict(model[not_in_lexicon])\n",
        "\n",
        "sents_df = pd.DataFrame((not_in_lexicon, values)).T\n",
        "sents_df.columns = [\"Word\",\"Sentiment\"]\n",
        "sents_df.Sentiment = sents_df.Sentiment / max(np.abs(sents_df.Sentiment))"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TY918YVhEzz",
        "colab_type": "code",
        "outputId": "1296079b-1e06-4ec6-cdbf-59d1eee8e378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "sents_df.sort_values(by=\"Sentiment\")[:10]"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9706</th>\n",
              "      <td>suffocating</td>\n",
              "      <td>-0.815016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38901</th>\n",
              "      <td>ghoul</td>\n",
              "      <td>-0.791145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42282</th>\n",
              "      <td>walkaway</td>\n",
              "      <td>-0.754887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4156</th>\n",
              "      <td>bleedin</td>\n",
              "      <td>-0.721677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19157</th>\n",
              "      <td>sickened</td>\n",
              "      <td>-0.70775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37747</th>\n",
              "      <td>unbeliever</td>\n",
              "      <td>-0.707698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17798</th>\n",
              "      <td>gagged</td>\n",
              "      <td>-0.693469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5900</th>\n",
              "      <td>noose</td>\n",
              "      <td>-0.693341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22552</th>\n",
              "      <td>psychopath</td>\n",
              "      <td>-0.692592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3004</th>\n",
              "      <td>suffocate</td>\n",
              "      <td>-0.68093</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Word Sentiment\n",
              "9706   suffocating -0.815016\n",
              "38901        ghoul -0.791145\n",
              "42282     walkaway -0.754887\n",
              "4156       bleedin -0.721677\n",
              "19157     sickened  -0.70775\n",
              "37747   unbeliever -0.707698\n",
              "17798       gagged -0.693469\n",
              "5900         noose -0.693341\n",
              "22552   psychopath -0.692592\n",
              "3004     suffocate  -0.68093"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ul3t4caDS7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from operator import itemgetter \n",
        "def closest_n_words_sentiment(model,vector,regressor,dist_func = cosine_similarity,vocab=model.wv.vocab, n=5):\n",
        "  dist = []\n",
        "  my_keys=[]\n",
        "  for item in vocab:\n",
        "    dist.append(dist_func(model.wv.get_vector(item).reshape(1,-1), vector.reshape(1,-1))[0][0])\n",
        "    my_keys.append(item)\n",
        "  sorted_w=np.argsort(dist)\n",
        "  if dist_func==cosine_similarity:\n",
        "    top_words =  list(itemgetter(*sorted_w[-n:])(my_keys))\n",
        "  else:\n",
        "    top_words =  list(itemgetter(*sorted_w[:-n])(my_keys))\n",
        "  sentiment = 0\n",
        "  for word in top_words:\n",
        "    sentiment+=vocab[word]/n\n",
        "  return sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UVcXfEfRiXr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df71fb68-7807-4f43-8de0-b0e7d842098f"
      },
      "source": [
        "word='useless'\n",
        "closest_n_words_sentiment(model,model.wv[word],elastic,cosine_similarity,lexicon_dict),elastic.predict(model.wv.get_vector(word).reshape(1,-1))"
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.21219999999999997, array([-0.372992], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 398
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkoGr3PlDS7M",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Word Vectors\n",
        "In this section, you'll plot words on a 2D grid based on their inner similarity. We'll use the tSNE transformation to reduce dimensions from 300 to 2. You can get sample code from https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial or other tutorials online.\n",
        "\n",
        "Perform the following:\n",
        "- Keep only the 3,000 most frequent words (after removing stopwords)\n",
        "- For this list, compute for each word its relative abundance in each of the genres\n",
        "- Compute the ratio between the proportion of each word in each genre and the proportion of the word in the entire corpus (the background distribution)\n",
        "- Pick the top 50 words for each genre. These words give good indication for that genre. Join the words from all genres into a single list of top significant words. \n",
        "- Compute tSNE transformation to 2D for all words, based on their word vectors\n",
        "- Plot the list of the top significant words in 2D. Next to each word output its text. The color of each point should indicate the genre for which it is most significant.\n",
        "\n",
        "You might prefer to use a different number of points or a slightly different methodology for improved results.  \n",
        "Analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbMVq0qNDS7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SbnSC7ODS7Z",
        "colab_type": "text"
      },
      "source": [
        "## Text Classification\n",
        "In this section, you'll build a text classifier, determining the genre of a song based on its lyrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGEeaybmDS7a",
        "colab_type": "text"
      },
      "source": [
        "### Text classification using Bag-of-Words\n",
        "Build a Naive Bayes classifier based on the bag of Words.  \n",
        "You will need to divide your dataset into a train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsEJTVnIDS7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2rl50SfDS7f",
        "colab_type": "text"
      },
      "source": [
        "Show the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okiEXOpsDS7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOg1yu_GDS7q",
        "colab_type": "text"
      },
      "source": [
        "Show the classification report - precision, recall, f1 for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dteE_0j0DS7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcgF6PDlDS74",
        "colab_type": "text"
      },
      "source": [
        "### Text classification using Word Vectors\n",
        "#### Average word vectors\n",
        "Do the same, using a classifier that averages the word vectors of words in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0SyYFhfDS75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulgJ8N62DS79",
        "colab_type": "text"
      },
      "source": [
        "#### TfIdf Weighting\n",
        "Do the same, using a classifier that averages the word vectors of words in the document, weighting each word by its TfIdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqaXwuVNDS79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqsS7xvVDS8D",
        "colab_type": "text"
      },
      "source": [
        "### Text classification using ConvNet\n",
        "Do the same, using a ConvNet.  \n",
        "The ConvNet should get as input a 2D matrix where each column is an embedding vector of a single word, and words are in order. Use zero padding so that all matrices have a similar length.  \n",
        "Some songs might be very long. Trim them so you keep a maximum of 128 words (after cleaning stop words and rare words).  \n",
        "Initialize the embedding layer using the word vectors that you've trained before, but allow them to change during training.  \n",
        "\n",
        "Extra: Try training the ConvNet with 2 slight modifications:\n",
        "1. freezing the the weights trained using Word2vec (preventing it from updating)\n",
        "1. random initialization of the embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNI7UUJmDS8E",
        "colab_type": "text"
      },
      "source": [
        "You are encouraged to try this question on your own.  \n",
        "\n",
        "You might prefer to get ideas from the paper \"Convolutional Neural Networks for Sentence Classification\" (Kim 2014, [link](https://arxiv.org/abs/1408.5882)).\n",
        "\n",
        "There are several implementations of the paper code in PyTorch online (see for example [this repo](https://github.com/prakashpandey9/Text-Classification-Pytorch) for a PyTorch implementation of CNN and other architectures for text classification). If you get stuck, they might provide you with a reference for your own code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUS_CbDPDS8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vZFPp63DrFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut9_Vm5mDtJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}